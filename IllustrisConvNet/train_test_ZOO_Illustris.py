from keras import backend as K
from keras.utils.visualize_util import plot
import pandas as pd
import pickle

def train_convnet_GZOO(X,Y,ntrain,nval,test_name):

    ind=random.sample(range(0, ntrain+nval-1), ntrain+nval-1)
    X_train = X[ind[0:ntrain],:,:,:]
    X_val = X[ind[ntrain:ntrain+nval],:,:,:]
    Y_train = Y[ind[0:ntrain],:]
    Y_val = Y[ind[ntrain:ntrain+nval],:]


    ## Params
    # model params
    batch_size = 64
    nb_epoch = 25
    data_augmentation = True
    normalize = True
    y_normalization = False
    norm_constant = 255

    # SGD parameters
    lr=0.001   #0.01
    decay=0
    momentum=0.9   #0.9
    nesterov=True

    depth=32
    nb_dense = 64

    #output params
    verbose = 1

    #print("Test name is: " + test_name)

    # input image dimensions
    img_rows, img_cols = X_train.shape[2:4]
    img_channels = 3
    #print(img_rows, img_cols)

    ### Right shape for X
    X_train = X_train.reshape(X_train.shape[0], img_channels, img_rows, img_cols)
    X_val = X_val.reshape(X_val.shape[0], img_channels, img_rows, img_cols)

    #misc.imsave(pathin+"examples/X_0.jpeg",X_train[0,0,:,:])
    #misc.imsave(pathin+"examples/X_100.jpeg",X_train[100,0,:,:])
    ##print X_train[0,0,32,:]
    ##print X_train(1,0,:,:)
    ##print X_train[100,0,32,:]

    model = Sequential()
    model.add(Convolution2D(32, 6,6, border_mode='same',
                        input_shape=(img_channels, img_rows, img_cols)))
    model.add(Activation('relu'))
    #model.add(Dropout(0.5))

    model.add(Convolution2D(64, 5, 5, border_mode='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))

    model.add(Convolution2D(128, 2, 2, border_mode='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.5))

    model.add(Convolution2D(128, 3, 3, border_mode='same'))
    model.add(Activation('relu'))
    #model.add(Dropout(0.25))

    model.add(Flatten())

    model.add(MaxoutDense(output_dim=128))
    model.add(Dropout(0.5))

    model.add(MaxoutDense(output_dim=128))
    model.add(Dropout(0.5))

    model.add(Dense(nparams, activation='softmax'))
    #model.add(Dropout(0.25))

    rms=rmsprop(lr=lr)
    model.compile(loss='mse',optimizer='rmsprop')




    if not data_augmentation:
        #print('Not using data augmentation.')
        history = model.fit(X_train, Y_train,
                            batch_size=batch_size,
                            nb_epoch=nb_epoch,
                            validation_data=(X_val, Y_val),
                            shuffle=True,
                            verbose=verbose)
    else:
        #print('Using real-time data augmentation.')

        # this will do preprocessing and realtime data augmentation
        datagen = ImageDataGenerator(
            featurewise_center=False,
            samplewise_center=False,
            featurewise_std_normalization=False,
            samplewise_std_normalization=False,
            zca_whitening=False,
            rotation_range=360,
            width_shift_range=0.05,
            height_shift_range=0.05,
            horizontal_flip=True,
            vertical_flip=True,
            zoom_range=[0.75,1.3])


        datagen.fit(X_train)

        # fit the model on the batches generated by datagen.flow()
        history = model.fit_generator(datagen.flow(X_train, Y_train,
                                                   batch_size=batch_size),
                                                   samples_per_epoch=X_train.shape[0],
                                                   nb_epoch=nb_epoch,
                                                   validation_data=(X_val, Y_val),
                                                   verbose=verbose)



    #print("Saving model...")
    model.save_weights(test_name+".hd5",overwrite=True)

    return

def validate_convnet_GZOO(X,model_name):

    ## Params
    # model params
    batch_size = 64
    nb_epoch = 50
    data_augmentation = True

    # SGD parameters
    lr=0.001   #0.01
    decay=1e-6
    momentum=0.9   #0.9
    nesterov=True
    nparams=3

    # input image dimensions
    img_rows, img_cols = X.shape[2:4]
    img_channels = 3
    #print(img_rows, img_cols)

    #print ("X", X.shape)
    ### Right shape for X
    X = X.reshape(X.shape[0], img_channels, img_rows, img_cols)
    #print ("X", X.shape)

    model = Sequential()
    # convolve with 32 6x6 filters - output is 32x6x6 for each sample
    model.add(Convolution2D(32,6,6, border_mode='same',
                        input_shape=(img_channels, img_rows, img_cols)))
    #rectified linear activation
    model.add(Activation('relu'))
    #model.add(Dropout(0.5))

    # convolve with 64 5x5 filters - output is 64x5x5 for each sample
    model.add(Convolution2D(64, 5, 5, border_mode='same'))
    #rectified linear activation
    model.add(Activation('relu'))
    # pool adjacent 2x2 groups replacing with max. val.
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))

    # convolve with 128 2x2 filters - output is 128x2x2 for each sample
    model.add(Convolution2D(128, 2, 2, border_mode='same'))
    #rectified linear activation
    model.add(Activation('relu'))
    # pool adjacent 2x2 groups replacing with max. val.
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.5))

    # convolve with 128 3x3 filters - output is 128x3x3 for each sample
    model.add(Convolution2D(128, 3, 3, border_mode='same'))
    #rectified linear activation
    model.add(Activation('relu'))
    #model.add(Dropout(0.25))

    model.add(Flatten())

    model.add(MaxoutDense(output_dim=128))
    model.add(Dropout(0.5))

    model.add(MaxoutDense(output_dim=128))
    model.add(Dropout(0.5))

    model.add(Dense(nparams, activation='softmax'))
    #model.add(Dropout(0.25))

    rms=rmsprop(lr=lr)
    model.compile(loss='mse',optimizer='rmsprop')

    ##load model weights
    model.load_weights(model_name+".hd5")

    '''for index, layer in enumerate(model.layers) :
        output = K.function([model.layers[0].input],[layer.output])([X])[0]
        #print(index, layer, output.shape)'''

    plot(model, to_file='model.png')

    Y_pred = model.predict(X, verbose=True)

    return Y_pred


def extract_thumb(im,x,y,size):
    if size %2==0:
        size = size+1
    #print(size)
    #print(x)
    #print(im.shape)
    up_x=int(x-size/2)
    dow_x=int(x+size/2)
    up_y=int(y-size/2)
    dow_y=int(y+size/2)
    res=im[up_x:dow_x,up_y:dow_y]
    #res=im[up_x:up_x + 64,up_y:up_y + 64]
    return res

def getHaloIds() :
    basedir = '/Volumes/lucifer_willett_data/gz4/illustris/png/'
    subdirs = glob.glob(basedir+'*')
    allFiles = []
    for subdir in subdirs :
        allFiles.extend(glob.glob(subdir + '/*.png'))
    allFileHaloIds = np.array([int(haloFile.split('/')[-1].split('_')[2]) for haloFile in allFiles])


def read_data(pathin,maxim = None, halo_ids = None, dryrun = False, annotation = 'GZ'):
    size_im=69
    size_crop=207

    nparams=3
    iteri=-1;
    numim=0;
    numim_init=numim

    basedir = '/Volumes/lucifer_willett_data/gz4/illustris/png/'
    subdirs = glob.glob(basedir+'*')
    allFiles = []
    for subdir in subdirs :
        allFiles.extend(glob.glob(subdir + '/*.png'))

    maxim = len(halo_ids) if halo_ids is not None and maxim is None else maxim
    maxim = len(allFiles) if maxim is None else maxim
    print ('Maximum number of images to process is {}'.format(maxim))
    D=np.zeros([maxim+1,3,size_im,size_im])

    numskipped = 0
    numaccepted = 0
    numfailed = 0

    #halo_ids = np.unique(halo_ids)
    allFileHaloIds = np.array([int(haloFile.split('/')[-1].split('_')[2]) for haloFile in allFiles])
    print(len(np.unique(halo_ids)), len(np.unique(allFileHaloIds)))

    haloIdList = []

    if dryrun :
        print('Running in dryrun mode. Only an ordered list of halo IDs will be generated.')

    while numaccepted<maxim :
        halo_id = int(allFiles[iteri+1].split('/')[-1].split('_')[2])
        #print (halo_id)
        if halo_ids is not None and halo_id not in halo_ids :
            #print('yes')
            iteri += 1
            numskipped += 1
            continue

        # IN "DRYRUN" MODE JUST BUILD A LIST OF HALO IDS IN THE ORDER THEY WOULD BE APPENDED TO
        # THE OUTPUT DATA CUBE. DO NOT LOAD OR PROCESS IMAGE DATA.
        if not dryrun :
            try:
                fileName=allFiles[iteri + 1] # note iteri is initialized to -1
                scidata = misc.imread(fileName, mode='RGB')
            except Exception as e:
                numfailed += 1
                continue

            lx,ly,lz=scidata.shape

            if lx<size_im:
                numfailed += 1
                continue

            scidata = extract_thumb(scidata,int(lx/2),int(ly/2),size_crop)
            scidata=zoom(scidata, [1/3.,1./3,1], order=3)
            scidata = np.transpose(scidata)

        iteri += 1
        numaccepted += 1
        if iteri % 500 == 0 :
            print ('Accepted {} of {} images (skipped {} = {} - {})...'.format(numaccepted, maxim, numskipped, iteri+1, numaccepted))

        if not dryrun :
            D[numaccepted,:,:,:] = scidata

        haloIdList.append((halo_id, allFiles[iteri]))

        numim=numim+1
    if not dryrun :
        np.save(pathin+"/image_vector"+annotation+".npy",D)
    return np.array(haloIdList)

from astropy.io import fits
from astropy.table import Table
from math import log10
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from astropy.modeling.models import Sersic2D
import os
from scipy.misc import imresize
os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=cpu,floatX=float32,exception_verbosity=high,optimizer=None"
from scipy import misc
from scipy.ndimage import zoom
from scipy.ndimage.interpolation import rotate
import glob
import theano
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten, MaxoutDense
from keras.engine.topology import Merge
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import rmsprop
import random

def run(pathin, npred = 1024) :

    ntrainv=[50000]

    for i in ntrainv:
        maxim=60000
        ntrain=i
        nval=i/10
        pred_index=0
        # pathin="/Users/hughdickinson/Documents/Development/IllustrisConvNet/"
        nparams=3

        #read_data(pathin,maxim)

        D=np.load(pathin+"/image_vectorGZ.npy")
        #Y=np.load(pathin+"target_vector"+str(maxim)+".npy")

        ##normalization
        mu = np.amax(D,axis=(2,3))
        sigma = np.std(D,axis=(2,3))
        #print (mu.shape)
        for i in range(0,mu.shape[0]):
            ##print i
            D[i,0,:,:] = D[i,0,:,:]/mu[i,0]
            D[i,1,:,:] = D[i,1,:,:]/mu[i,1]
            D[i,2,:,:] = D[i,2,:,:]/mu[i,2]

        mu_y=np.zeros(nparams)
        sigma_y=np.zeros(nparams)+1.0

        for x in range(0,nparams):
            #mu_y[x] = np.mean(Y[:,x])
            #sigma_y[x] = np.std(Y[:,x])
            mu_y[x]=0
            sigma_y[x]=1
            #Y[:,x] = (Y[:,x] - mu_y[x]) / sigma_y[x]


        #print (D.shape)

        #train_convnet_GZOO(D,Y,ntrain,nval,pathin+"ZOO_model_"+str(ntrain))
        Y_pred=validate_convnet_GZOO(D[pred_index:pred_index+npred,:,:,:],pathin+"/ZOO_model_"+str(ntrain))
        Y_val=np.copy(Y_pred)
        Y_pred[:,0]=Y_pred[:,0]*sigma_y[0]+mu_y[0]
        Y_pred[:,1]=Y_pred[:,1]*sigma_y[1]+mu_y[1]
        Y_pred[:,2]=Y_pred[:,2]*sigma_y[2]+mu_y[2]
        #Y_val[:,0]=Y[pred_index:pred_index+npred,0]*sigma_y[0]+mu_y[0]
        #Y_val[:,1]=Y[pred_index:pred_index+npred,1]*sigma_y[1]+mu_y[1]
        #Y_val[:,2]=Y[pred_index:pred_index+npred,2]*sigma_y[2]+mu_y[2]
        #col1 = fits.Column(name='dr7objid', format='K', array=idvec_val)
        col2 = fits.Column(name='a0_1_pred', format='F', array=Y_pred[:,0])
        col3 = fits.Column(name='a0_2_pred', format='F', array=Y_pred[:,1])
        col4 = fits.Column(name='a0_3_pred', format='F', array=Y_pred[:,2])
        col5 = fits.Column(name='a0_1_in', format='F', array=Y_val[:,0])
        col6 = fits.Column(name='a0_2_in', format='F', array=Y_val[:,1])
        col7 = fits.Column(name='a0_3_in', format='F', array=Y_val[:,2])

        cols = fits.ColDefs([col2,col3,col4,col5,col6,col7])
        tbhdu = fits.BinTableHDU.from_columns(cols)
        tbhdu.writeto(pathin+"/catalog_DL_3params_single"+str(ntrain)+".fit",clobber='True')

        return Y_pred

if __name__ == '__main__':
    run()
